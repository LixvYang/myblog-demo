import{_ as r}from"./plugin-vue_export-helper-c27b6911.js";import{r as p,o as a,c as d,a as e,b as o,d as i,f as n}from"./app-5bba858b.js";const t="/assets/images/MySQL/Buffer-Pool-Pages.png",c={},g=n('<h1 id="mysql-学习笔记" tabindex="-1"><a class="header-anchor" href="#mysql-学习笔记" aria-hidden="true">#</a> MySQL 学习笔记</h1><ul><li><a href="#%E6%89%A7%E8%A1%8C%E4%B8%80%E6%9D%A1SQL%E6%9F%A5%E8%AF%A2%E8%AF%AD%E5%8F%A5%EF%BC%8C%E6%9C%9F%E9%97%B4%E5%8F%91%E7%94%9F%E4%BA%86%E4%BB%80%E4%B9%88">执行一条 SQL 查询语句，期间发生了什么？</a></li><li><a href="#%E7%B4%A2%E5%BC%95">索引</a></li><li><a href="#%E5%86%85%E5%AD%98">内存</a></li><li><a href="#%E6%97%A5%E5%BF%97">日志</a></li><li><a href="#%E4%BA%8B%E5%8A%A1">事务</a></li><li><a href="#%E9%94%81">锁</a></li><li><a href="#BufferPool">说清楚Buffer Pool是啥</a></li></ul><p><span id="执行一条SQL查询语句，期间发生了什么"></span></p><h2 id="执行一条sql查询语句-期间发生了什么" tabindex="-1"><a class="header-anchor" href="#执行一条sql查询语句-期间发生了什么" aria-hidden="true">#</a> 执行一条SQL查询语句，期间发生了什么</h2><p>当MySQL执行一条查询语句时,中间发生了什么?</p><p>带着这个问题，我们可以很好的了解 MySQL 内部的架构，所以这次小林就带大家拆解一下 MySQL 内部的结构，看看内部里的每一个“零件”具体是负责做什么的。</p>',6),u={href:"https://cdn.xiaolincoding.com/gh/xiaolincoder/mysql/sql%E6%89%A7%E8%A1%8C%E8%BF%87%E7%A8%8B/mysql%E6%9F%A5%E8%AF%A2%E6%B5%81%E7%A8%8B.png",target:"_blank",rel:"noopener noreferrer"},s=n(`<p>可以看到MySQL的架构总共分为两层, Server层和引擎层.</p><p>Server负责建立连接,查询缓存,解析,预处理,优化和执行,这其中也包括所有的内置函数（如日期、时间、数学和加密函数等）和所有跨存储引擎的功能（如存储过程、触发器、视图等。）都在 Server 层实现。</p><p>引擎层是负责数据的存储和提取. 支持InnoDB, MyISAM, Memory等多个存储引擎,我们常说的索引数据结构，就是由存储引擎层实现的，不同的存储引擎支持的索引类型也不相同，比如 InnoDB 支持索引类型是 B+树 ，且是默认使用，也就是说在数据表中创建的主键索引和二级索引默认使用的是 B+ 树索引。</p><h3 id="连接器" tabindex="-1"><a class="header-anchor" href="#连接器" aria-hidden="true">#</a> 连接器</h3><p>连接器负责的工作有:</p><ol><li>与客户端进行 TCP 三次握手建立连接；</li><li>校验客户端的用户名和密码，如果用户名或密码不对，则会报错；</li><li>如果用户名和密码都对了，会读取该用户的权限，然后后面的权限逻辑判断都基于此时读取到的权限；</li></ol><h3 id="查询缓存" tabindex="-1"><a class="header-anchor" href="#查询缓存" aria-hidden="true">#</a> 查询缓存</h3><blockquote><p>查询缓存在MySQL8.0版本之后就已经去除了(可能是确实效率低)</p></blockquote><p>连接器得工作完成后，客户端就可以向 MySQL 服务发送 SQL 语句了，MySQL 服务收到 SQL 语句后，就会解析出 SQL 语句的第一个字段，看看是什么类型的语句。</p><p>如果 SQL 是查询语句（select 语句），MySQL 就会先去查询缓存（ Query Cache ）里查找缓存数据，看看之前有没有执行过这一条命令，这个查询缓存是以 key-value 形式保存在内存中的，key 为 SQL 查询语句，value 为 SQL 语句查询的结果。</p><p>如果查询的语句命中查询缓存，那么就会直接返回 value 给客户端。如果查询的语句没有命中查询缓存中，那么就要往下继续执行，等执行完后，查询的结果就会被存入查询缓存中。</p><h3 id="解析-sql" tabindex="-1"><a class="header-anchor" href="#解析-sql" aria-hidden="true">#</a> 解析 SQL</h3><h4 id="解析器" tabindex="-1"><a class="header-anchor" href="#解析器" aria-hidden="true">#</a> 解析器</h4><p>解析器会做如下两件事情。</p><p>第一件事情，词法分析。MySQL 会根据你输入的字符串识别出关键字出来，构建出 SQL 语法树，这样方便后面模块获取 SQL 类型、表名、字段名、 where 条件等等。</p><p>第二件事情，语法分析。根据词法分析的结果，语法解析器会根据语法规则，判断你输入的这个 SQL 语句是否满足 MySQL 语法。</p><p>如果我们输入的 SQL 语句语法不对，就会在解析器这个阶段报错。比如，我下面这条查询语句，把 from 写成了 form，这时 MySQL 解析器就会给报错。</p><h3 id="执行-sql" tabindex="-1"><a class="header-anchor" href="#执行-sql" aria-hidden="true">#</a> 执行 SQL</h3><h4 id="预处理器" tabindex="-1"><a class="header-anchor" href="#预处理器" aria-hidden="true">#</a> 预处理器</h4><p>我们先来说说预处理阶段做了什么事情。</p><ol><li>检查 SQL 查询语句中的表或者字段是否存在；</li><li>将 select * 中的 * 符号，扩展为表上的所有列；</li></ol><h4 id="优化器" tabindex="-1"><a class="header-anchor" href="#优化器" aria-hidden="true">#</a> 优化器</h4><p>经过预处理阶段后，还需要为 SQL 查询语句先制定一个执行计划，这个工作交由「优化器」来完成的。</p><p>优化器主要负责将 SQL 查询语句的执行方案确定下来，比如在表里面有多个索引的时候，优化器会基于查询成本的考虑，来决定选择使用哪个索引。</p><h4 id="执行器" tabindex="-1"><a class="header-anchor" href="#执行器" aria-hidden="true">#</a> 执行器</h4><ul><li>主键索引查询</li><li>全表扫描</li><li>索引下推</li></ul><p>总结</p><p>执行一条 SQL 查询语句，期间发生了什么？</p><pre><code>连接器：建立连接，管理连接、校验用户身份；
查询缓存：查询语句如果命中查询缓存则直接返回，否则继续往下执行。MySQL 8.0 已删除该模块；
解析 SQL，通过解析器对 SQL 查询语句进行词法分析、语法分析，然后构建语法树，方便后续模块读取表名、字段、语句类型；
执行 SQL：执行 SQL 共有三个阶段：
    预处理阶段：检查表或字段是否存在；将 select * 中的 * 符号扩展为表上的所有列。
    优化阶段：基于查询成本的考虑， 选择查询成本最小的执行计划；
    执行阶段：根据执行计划执行 SQL 查询语句，从存储引擎读取记录，返回给客户端；
</code></pre><p><span id="索引"></span></p><h2 id="索引常见问题" tabindex="-1"><a class="header-anchor" href="#索引常见问题" aria-hidden="true">#</a> 索引常见问题</h2><p>当你想查阅书中某个知识的内容，你会选择一页一页的找呢？还是在书的目录去找呢？</p><p>傻瓜都知道时间是宝贵的，当然是选择在书的目录去找，找到后再翻到对应的页。书中的目录，就是充当索引的角色，方便我们快速查找书中的内容，所以索引是以空间换时间的设计思想。</p><p>那换到数据库中，索引的定义就是帮助存储引擎快速获取数据的一种数据结构，形象的说就是索引是数据的目录。</p><p>所谓的存储引擎，说白了就是如何存储数据、如何为存储的数据建立索引和如何更新、查询数据等技术的实现方法。MySQL 存储引擎有 MyISAM 、InnoDB、Memory，其中 InnoDB 是在 MySQL 5.5 之后成为默认的存储引擎。</p><p>索引最大的好处就是可以提高查询速度, 但也是有问题的, 比如</p><ul><li>需要占用物理空间，数量越大，占用空间越大；</li><li>创建索引和维护索引要耗费时间，这种时间随着数据量的增加而增大；</li><li>会降低表的增删改的效率，因为每次增删改索引，B+ 树为了维护索引有序性，都需要进行动态维护。</li></ul><h3 id="什么时候需要创建索引" tabindex="-1"><a class="header-anchor" href="#什么时候需要创建索引" aria-hidden="true">#</a> 什么时候需要创建索引</h3><ul><li>字段有唯一性限制的，比如商品编码；</li><li>经常用于 WHERE 查询条件的字段，这样能够提高整个表的查询速度，如果查询条件不是一个字段，可以建立联合索引。</li><li>经常用于 GROUP BY 和 ORDER BY 的字段，这样在查询的时候就不需要再去做一次排序了，因为我们都已经知道了建立索引之后在 B+Tree 中的记录都是排序好的。</li></ul><h3 id="什么时候不需要创建索引" tabindex="-1"><a class="header-anchor" href="#什么时候不需要创建索引" aria-hidden="true">#</a> 什么时候不需要创建索引</h3><ul><li>WHERE 条件，GROUP BY，ORDER BY 里用不到的字段，索引的价值是快速定位，如果起不到定位的字段通常是不需要创建索引的，因为索引是会占用物理空间的。 字段中存在大量重复数据，不需要创建索引，比如性别字段，只有男女，如果数据库表中，男女的记录分布均匀，那么无论搜索哪个值都可能得到一半的数据。在这些情况下，还不如不要索引，因为 MySQL 还有一个查询优化器，查询优化器发现某个值出现在表的数据行中的百分比很高的时候，它一般会忽略索引，进行全表扫描。</li><li>表数据太少的时候，不需要创建索引；</li><li>经常更新的字段不用创建索引，比如不要对电商项目的用户余额建立索引，因为索引字段频繁修改，由于要维护 B+Tree的有序性，那么就需要频繁的重建索引，这个过程是会影响数据库性能的。</li></ul><h3 id="有什么优化索引的方法" tabindex="-1"><a class="header-anchor" href="#有什么优化索引的方法" aria-hidden="true">#</a> 有什么优化索引的方法？</h3><p>这里说一下几种常见优化索引的方法：</p><ul><li>前缀索引优化；</li><li>覆盖索引优化；</li><li>主键索引最好是自增的；</li><li>防止索引失效；</li></ul><ol><li>前缀索引优化； 前缀索引顾名思义就是使用某个字段中字符串的前几个字符建立索引，那我们为什么需要使用前缀来建立索引呢？</li></ol><p>使用前缀索引是为了减小索引字段大小，可以增加一个索引页中存储的索引值，有效提高索引的查询速度。在一些大字符串的字段作为索引时，使用前缀索引可以帮助我们减小索引项的大小。</p><p>不过，前缀索引有一定的局限性，例如：</p><pre><code>order by 就无法使用前缀索引；
无法把前缀索引用作覆盖索引；
</code></pre><ol start="2"><li>覆盖索引优化；</li></ol><p>覆盖索引是指 SQL 中 query 的所有字段，在索引 B+Tree 的叶子节点上都能找得到的那些索引，从二级索引中查询得到记录，而不需要通过聚簇索引查询获得，可以避免回表的操作。</p><p>假设我们只需要查询商品的名称、价格，有什么方式可以避免回表呢？</p><p>我们可以建立一个联合索引，即「商品ID、名称、价格」作为一个联合索引。如果索引中存在这些数据，查询将不会再次检索主键索引，从而避免回表。</p><p>所以，使用覆盖索引的好处就是，不需要查询出包含整行记录的所有信息，也就减少了大量的 I/O 操作。</p><ol start="3"><li>主键索引最好是自增的</li></ol><p>如果我们使用自增主键，那么每次插入的新数据就会按顺序添加到当前索引节点的位置，不需要移动已有的数据，当页面写满，就会自动开辟一个新页面。因为每次插入一条新记录，都是追加操作，不需要重新移动数据，因此这种插入数据的方法效率非常高。</p><p>如果我们使用非自增主键，由于每次插入主键的索引值都是随机的，因此每次插入新的数据时，就可能会插入到现有数据页中间的某个位置，这将不得不移动其它数据来满足新数据的插入，甚至需要从一个页面复制数据到另外一个页面，我们通常将这种情况称为页分裂。页分裂还有可能会造成大量的内存碎片，导致索引结构不紧凑，从而影响查询效率。</p><ol start="4"><li>防止索引失效</li></ol><p>这里简单说一下，发生索引失效的情况：</p><ul><li>当我们使用左或者左右模糊匹配的时候，也就是 like %xx 或者 like %xx%这两种方式都会造成索引失效；</li><li>当我们在查询条件中对索引列做了计算、函数、类型转换操作，这些情况下都会造成索引失效；</li><li>联合索引要能正确使用需要遵循最左匹配原则，也就是按照最左优先的方式进行索引的匹配，否则就会导致索引失效。</li><li>在 WHERE 子句中，如果在 OR 前的条件列是索引列，而在 OR 后的条件列不是索引列，那么索引会失效。</li></ul><figure><img src="https://cdn.xiaolincoding.com/gh/xiaolincoder/mysql/索引/索引总结.drawio.png" alt="" tabindex="0" loading="lazy"><figcaption></figcaption></figure>`,60),h={href:"https://xiaolincoding.com/mysql/index/page.html#innodb-%E6%98%AF%E5%A6%82%E4%BD%95%E5%AD%98%E5%82%A8%E6%95%B0%E6%8D%AE%E7%9A%84",target:"_blank",rel:"noopener noreferrer"},b={href:"https://xiaolincoding.com/mysql/index/why_index_chose_bpuls_tree.html",target:"_blank",rel:"noopener noreferrer"},f={href:"https://xiaolincoding.com/mysql/index/index_lose.html",target:"_blank",rel:"noopener noreferrer"},y=e("p",null,[e("span",{id:"内存"})],-1),_=e("h2",{id:"内存-揭开-buffer-pool-的面纱",tabindex:"-1"},[e("a",{class:"header-anchor",href:"#内存-揭开-buffer-pool-的面纱","aria-hidden":"true"},"#"),o(" 内存 揭开 Buffer Pool 的面纱")],-1),m={href:"https://xiaolincoding.com/mysql/buffer_pool/buffer_pool.html",target:"_blank",rel:"noopener noreferrer"},L=n('<h2 id="" tabindex="-1"><a class="header-anchor" href="#" aria-hidden="true">#</a></h2><p><span id="日志"></span></p><h2 id="日志" tabindex="-1"><a class="header-anchor" href="#日志" aria-hidden="true">#</a> 日志</h2><ul><li>undo log（回滚日志）：是 Innodb 存储引擎层生成的日志，实现了事务中的原子性，主要用于事务回滚和 MVCC。</li><li>redo log（重做日志）：是 Innodb 存储引擎层生成的日志，实现了事务中的持久性，主要用于掉电等故障恢复；</li><li>binlog （归档日志）：是 Server 层生成的日志，主要用于数据备份和主从复制；</li></ul>',4),x={href:"https://xiaolincoding.com/mysql/log/how_update.html",target:"_blank",rel:"noopener noreferrer"},S=n(`<p>当执行一条update的时候,发生了什么?</p><p>首先,客户端通过连接器建立连接,连接器会判断用户身份,因为这是一条update语句,以不需要经过查询缓存，但是表上有更新语句，是会把整个表的查询缓存清空的，所以说查询缓存很鸡肋,所以MySQL8.0就被移除了,之后会经过词法分析和语法分析,词法分析的主要作用是识别出关键词update,表名等等,语法分析是判断是否符合MySQL语法. 之后会经过预处理器,预处理器会判断表名和字段是否存在,优化器确定执行计划，因为 where 条件中的 id 是主键索引，所以决定要使用 id 这个索引；执行器负责具体执行,找到这一行,然后更新.</p><p>不过更新语句还涉及到undo log(回滚日志),redo log(重做日志),binlog(归档日志)这三种日志:</p><ul><li>undo log(回滚日志): 是InnoDB存储引擎层生成的日志,实现了日志的原子性,主要用于事务的回滚.</li><li>redo log,是InnoDB存储引擎层生成的日志,实现了事务中的持久性,主要用于掉电等故障恢复</li><li>bin log,是Server生成的日志,主要用于数据恢复和主从复制.</li></ul><p>这一小节的问题包括:</p><ul><li>为什么需要undo log</li><li>为什么需要Buffer Pool</li><li>为什么需要redo log</li><li>为什么需要bin log</li><li>为什么需要两阶段提交</li><li>两阶段提交有什么问题</li><li>MySQL磁盘I/O很高有什么优化点</li></ul><h3 id="为什么需要undo-log" tabindex="-1"><a class="header-anchor" href="#为什么需要undo-log" aria-hidden="true">#</a> 为什么需要undo log</h3><p>我们在MySQL中实现<strong>增删改</strong>语句的时候,MySQL会判断<code>autocommit</code>参数来决定是否默认开启事务,如果默认开启,执行一条 update 语句也是会使用事务的。在事务执行过程中,都会记录下回滚所需要的信息到一个日志里,那么事务发生过程中MySQL崩溃了,或者事务执行失败,我们就可以通过这个日志回到事务发生之前的数据,这就叫Undo log,并且它保证了事务的ACID中的原子性.</p><p>一条记录的每一次更新操作产生的undo log格式都有一个roll_potiner指针和一个trx_id事务id:</p><ul><li>trx_id指针可以知道该记录是被哪一个事务修改的</li><li>roll_potiner指针可以将这些 undo log 串成一个链表，这个链表就被称为版本链</li></ul><p>另外，undo log 还有一个作用，通过 ReadView + undo log 实现 MVCC（多版本并发控制）</p><p>对于读提交隔离级别的事务而言,每次在事务执行中的select都会生成一个新的ReadView,这意味着事务期间多次读取同一条数据,前后两次读取的数据可能不一致可能会发生不可重复读.对于可重复读隔离级别的事务是启动事务时生成一个 Read View,然后整个事务期间都在用这个 Read View，这样就保证了在事务期间读到的数据都是事务启动前的记录。</p><p>这两种隔离级别的实现是通过事务的Read View里的字段和记录中的两个隐藏列(trx_id和roll_pointer)的对比如果不满足可见行,就会顺着undolog版本链里面找到可见性的记录,从而控制并发事务访问同一个记录时的行为，这就叫 MVCC（多版本并发控制）。</p><p>undo log 两大作用：</p><ul><li>实现事务回滚，保障事务的原子性。事务处理过程中，如果出现了错误或者用户执 行了 ROLLBACK 语句，MySQL 可以利用 undo log 中的历史数据将数据恢复到事务开始之前的状态。</li><li>实现 MVCC（多版本并发控制）关键因素之一。MVCC 是通过 ReadView + undo log 实现的。undo log 为每条记录保存多份历史数据，MySQL 在执行快照读（普通 select 语句）的时候，会根据事务的 Read View 里的信息，顺着 undo log 的版本链找到满足其可见性的记录.</li></ul><blockquote><p>Undo log是如何刷盘的?undo log 和数据页的刷盘策略是一样的，都需要通过 redo log 保证持久化。buffer pool 中有 undo 页，对 undo 页的修改也都会记录到 redo log。redo log 会每秒刷盘，提交事务时也会刷盘，数据页和 undo 页都是靠这个机制保证持久化的。</p></blockquote><h3 id="为什么需要-redo-log" tabindex="-1"><a class="header-anchor" href="#为什么需要-redo-log" aria-hidden="true">#</a> 为什么需要 redo log ？</h3><p>redo log是在数据被修改后,或者事物执行后将数据被修改后的数据记录到log里,记录的是某个数据页面做了的修改,比如对 XXX 表空间中的 YYY 数据页 ZZZ 偏移量的地方做了AAA 更新，每当执行一个事务就会产生这样的一条或者多条物理日志。</p><p>为了防止断电后导致的数据丢失问题,当需要有一条记录需要更新时,InnoDB就会先更新内存,（同时标记为脏页），然后将本次对这个页的修改以 redo log 的形式记录下来，这个时候更新就算完成了。后续InnoDB会进行更新,由后台线程将缓存在Buffer Pool里的脏页刷新到磁盘里,这就是WAL技术.WAL技术是指MySQL的写操作并不是立即写到磁盘上,而是先写日志,然后在之后合适的时间再写到磁盘上.</p><p>这样即使后面系统崩溃了,也可以通过redo log来恢复数据.</p><blockquote><p>被修改 Undo 页面，需要记录对应 redo log 吗？</p></blockquote><p>需要的。开启事务后，InnoDB 层更新记录前，首先要记录相应的 undo log，如果是更新操作，需要把被更新的列的旧值记下来，也就是要生成一条 undo log，undo log 会写入 Buffer Pool 中的 Undo 页面。不过，在内存修改该 Undo 页面后，需要记录对应的 redo log。</p><blockquote><pre><code>redo log 和 undo log 区别在哪？
</code></pre></blockquote><p>redo log 记录了此次事务「完成后」的数据状态，记录的是更新之后的值； undo log 记录了此次事务「开始前」的数据状态，记录的是更新之前的值；</p><p>事务提交之前发生了崩溃，重启后会通过 undo log 回滚事务，事务提交之后发生了崩溃，重启后会通过 redo log 恢复事务</p><figure><img src="https://cdn.xiaolincoding.com/gh/xiaolincoder/mysql/how_update/事务恢复.png?image_process=watermark,text_5YWs5LyX5Y-377ya5bCP5p6XY29kaW5n,type_ZnpsdHpoaw,x_10,y_10,g_se,size_20,color_0000CD,t_70,fill_0" alt="" tabindex="0" loading="lazy"><figcaption></figcaption></figure><p>所以有了 redo log，再通过 WAL 技术，InnoDB 就可以保证即使数据库发生异常重启，之前已提交的记录都不会丢失，这个能力称为 crash-safe（崩溃恢复）。可以看出来， redo log 保证了事务四大特性中的持久性。</p><blockquote><p>redo log写到磁盘,数据也写到磁盘,会不会麻烦? 不麻烦,redo log在磁盘中是顺序写,而写入数据则是随机写,性能差别巨大,可以说这是WAL技术的另一个优点,MySQL 的写操作从磁盘的「随机写」变成了「顺序写」，提升语句的执行性能。</p></blockquote><p>所以为什么要有redo log?</p><ol><li>实现事务的持久性,让MySQL有了crash-safe 的能力，能够保证 MySQL 在任何时间段突然崩溃，重启后之前已提交的记录都不会丢失</li><li>将写操作从「随机写」变成了「顺序写」，提升 MySQL 写入磁盘的性能。</li></ol><blockquote><p>产生的 redo log 是直接写入磁盘的吗？</p></blockquote><p>其实也不是</p><p>实际上在执行一个事务的过程中,产生的redo log也不是直接写入磁盘的,因为这样会产生大量的 I/O 操作，而且磁盘的运行速度远慢于内存。</p><p>所以redo log也有自己的redo log buffer,每当产生一条redo log时会先写到redo log buffer中,后续会持久化.redo log buffer默认16MB,可以通过 <code>innodb_log_Buffer_size</code> 参数动态的调整大小，增大它的大小可以让 MySQL 处理「大事务」是不必写入磁盘，进而提升写 IO 性能。</p><blockquote><p>redo log什么时候刷盘?</p></blockquote><p>缓存在 redo log buffer 里的 redo log 还是在内存中，它什么时候刷新到磁盘？</p><ol><li>MySQL正常关机</li><li>InnoDB后台线程每秒刷新一次</li><li>当redo log buffer记录的写入量大于redo log buffer内存空间的一半时.</li><li>每次事务提交时都将缓存在 redo log buffer 里的 redo log 直接持久化到磁盘</li></ol><p>innodb_flush_log_at_trx_commit 参数控制的是什么？</p><p>innodb_flush_log_at_trx_commit可以控制redo log buffer的持久策略. 参数为0.表示每次事务提交时,将redo log保留在redo log buffer中,这一模式事务提交不会主动刷盘,数据完整性较低,可能会发生丢失数据 参数为1.表示每次数据提交时候,都会将缓存在 redo log buffer 里的 redo log 直接持久化到磁盘,这样MySQL即使异常重启也不会丢失数据 参数为2.表示表示每次事务提交时，都只是缓存在 redo log buffer 里的 redo log 写到 redo log 文件，注意写入到「 redo log 文件」并不意味着写入到了磁盘,因为操作系统的文件系统中有个 Page Cache，Page Cache 是专门用来缓存文件数据的，所以写入「 redo log文件」意味着写入到了操作系统的文件缓存。</p><p>这三个参数的数据安全性和写入性能的比较如下：</p><pre><code>数据安全性：参数 1 &gt; 参数 2 &gt; 参数 0
写入性能：参数 0 &gt; 参数 2&gt; 参数 1
</code></pre><figure><img src="https://cdn.xiaolincoding.com/gh/xiaolincoder/mysql/how_update/innodb_flush_log_at_trx_commit.drawio.png?image_process=watermark,text_5YWs5LyX5Y-377ya5bCP5p6XY29kaW5n,type_ZnpsdHpoaw,x_10,y_10,g_se,size_20,color_0000CD,t_70,fill_0" alt="" tabindex="0" loading="lazy"><figcaption></figcaption></figure><blockquote><p>redo log 文件写满了怎么办？</p></blockquote><p>默认情况下， InnoDB 存储引擎有 1 个重做日志文件组( redo log Group），「重做日志文件组」由有 2 个 redo log 文件组成，这两个 redo 日志的文件名叫 ：ib_logfile0 和 ib_logfile1 。</p><figure><img src="https://cdn.xiaolincoding.com/gh/xiaolincoder/mysql/how_update/重做日志文件组.drawio.png" alt="" tabindex="0" loading="lazy"><figcaption></figcaption></figure><p>在重做日志组中，每个 redo log File 的大小是固定且一致的，假设每个 redo log File 设置的上限是 1 GB，那么总共就可以记录 2GB 的操作。重做日志文件组是以循环写的方式工作的，从头开始写，写到末尾就又回到开头，相当于一个环形。redo log 是循环写的方式，相当于一个环形，InnoDB 用 write pos 表示 redo log 当前记录写到的位置，用 checkpoint 表示当前要擦除的位置，如下图：</p><figure><img src="https://cdn.xiaolincoding.com/gh/xiaolincoder/mysql/how_update/checkpoint.png" alt="" tabindex="0" loading="lazy"><figcaption></figcaption></figure><p>图中的：</p><ul><li>write pos 和 checkpoint 的移动都是顺时针方向；</li><li>write pos ～ checkpoint 之间的部分（图中的红色部分），用来记录新的更新操作；</li><li>check point ～ write pos 之间的部分（图中蓝色部分）：待落盘的脏数据页记录；</li></ul><p>如果 write pos 追上了 checkpoint，就意味着 redo log 文件满了，这时 MySQL 不能再执行新的更新操作，也就是说 MySQL 会被阻塞（因此所以针对并发量大的系统，适当设置 redo log 的文件大小非常重要），此时会停下来将 Buffer Pool 中的脏页刷新到磁盘中，然后标记 redo log 哪些记录可以被擦除，接着对旧的 redo log 记录进行擦除，等擦除完旧记录腾出了空间，checkpoint 就会往后移动（图中顺时针），然后 MySQL 恢复正常运行，继续执行新的更新操作。</p><p>所以，一次 checkpoint 的过程就是脏页刷新到磁盘中变成干净页，然后标记 redo log 哪些记录可以被覆盖的过程。</p><h3 id="为什么需要-binlog" tabindex="-1"><a class="header-anchor" href="#为什么需要-binlog" aria-hidden="true">#</a> 为什么需要 binlog ？</h3><p>MySQL 在完成一条更新操作后，Server 层还会生成一条 binlog，等之后事务提交的时候，会将该事物执行过程中产生的所有 binlog 统一写 入 binlog 文件。binlog 文件是记录了所有数据库表结构变更和表数据修改的日志，不会记录查询类的操作，比如 SELECT 和 SHOW 操作。</p><blockquote><pre><code>为什么有了 binlog， 还要有 redo log？
</code></pre></blockquote><p>这个问题跟 MySQL 的时间线有关系。最开始 MySQL 里并没有 InnoDB 引擎，MySQL 自带的引擎是 MyISAM，但是 MyISAM 没有 crash-safe 的能力，binlog 日志只能用于归档。而 InnoDB 是另一个公司以插件形式引入 MySQL 的，既然只依靠 binlog 是没有 crash-safe 能力的，所以 InnoDB 使用 redo log 来实现 crash-safe 能力。</p><blockquote><p>redo log 和 binlog 有什么区别？</p></blockquote><p>这两个日志有四个区别。</p><ol><li>适用对象不同： <ul><li>binlog 是 MySQL 的 Server 层实现的日志，所有存储引擎都可以使用；</li><li>redo log 是 Innodb 存储引擎实现的日志；</li></ul></li><li>文件格式不同： <ul><li>binlog 有 3 种格式类型，分别是 STATEMENT（默认格式）、ROW、 MIXED，区别如下： <ul><li>STATEMENT：每一条修改数据的 SQL 都会被记录到 binlog 中（相当于记录了逻辑操作，所以针对这种格式， binlog 可以称为逻辑日志），主从复制中 slave 端再根据 SQL 语句重现。但 STATEMENT 有动态函数的问题，比如你用了 uuid 或者 now 这些函数，你在主库上执行的结果并不是你在从库执行的结果，这种随时在变的函数会导致复制的数据不一致；</li><li>ROW：记录行数据最终被修改成什么样了（这种格式的日志，就不能称为逻辑日志了），不会出现 STATEMENT 下动态函数的问题。但 ROW 的缺点是每行数据的变化结果都会被记录，比如执行批量 update 语句，更新多少行数据就会产生多少条记录，使 binlog 文件过大，而在 STATEMENT 格式下只会记录一个 update 语句而已；</li><li>MIXED：包含了 STATEMENT 和 ROW 模式，它会根据不同的情况自动使用 ROW 模式和 STATEMENT 模式；</li></ul></li><li>redo log 是物理日志，记录的是在某个数据页做了什么修改，比如对 XXX 表空间中的 YYY 数据页 ZZZ 偏移量的地方做了AAA 更新；</li></ul></li><li>写入方式不同 <ul><li>binlog 是追加写，写满一个文件，就创建一个新的文件继续写，不会覆盖以前的日志，保存的是全量的日志。</li><li>redo log 是循环写，日志空间大小是固定，全部写满就从头开始，保存未被刷入磁盘的脏页日志。</li></ul></li><li>用途不同： <ul><li>binlog 用于备份恢复、主从复制；</li><li>redo log 用于掉电等故障恢复。</li></ul></li></ol><blockquote><p>如果不小心整个数据库的数据被删除了，能使用 redo log 文件恢复数据吗？</p></blockquote><p>不可以使用 redo log 文件恢复，只能使用 binlog 文件恢复。</p><p>因为 redo log 文件是循环写，是会边写边擦除日志的，只记录未被刷入磁盘的数据的物理日志，已经刷入磁盘的数据都会从 redo log 文件里擦除。</p><p>binlog 文件保存的是全量的日志，也就是保存了所有数据变更的情况，理论上只要记录在 binlog 上的数据，都可以恢复，所以如果不小心整个数据库的数据被删除了，得用 binlog 文件恢复数据。</p><blockquote><p>binlog 什么时候刷盘？</p></blockquote><p>事务执行过程中，先把日志写到 binlog cache（Server 层的 cache），事务提交的时候，再把 binlog cache 写到 binlog 文件中。个事务的 binlog 是不能被拆开的，因此无论这个事务有多大（比如有很多条语句），也要保证一次性写入。这是因为有一个线程只能同时有一个事务在执行的设定，所以每当执行一个 begin/start transaction 的时候，就会默认提交上一个事务，这样如果一个事务的 binlog 被拆开的时候，在备库执行就会被当做多个事务分段自行，这样破坏了原子性，是有问题的。MySQL 给每个线程分配了一片内存用于缓冲 binlog ，该内存叫 binlog cache，参数 binlog_cache_size 用于控制单个线程内 binlog cache 所占内存的大小。如果超过了这个参数规定的大小，就要暂存到磁盘。</p><blockquote><pre><code>什么时候 binlog cache 会写到 binlog 文件？
</code></pre></blockquote><p>在事务提交的时候，执行器把 binlog cache 里的完整事务写入到 binlog 文件中，并清空 binlog cache。虽然每个线程有自己 binlog cache，但是最终都写到同一个 binlog 文件： <img src="https://cdn.xiaolincoding.com/gh/xiaolincoder/mysql/how_update/binlogcache.drawio.png" alt="" loading="lazy"></p><p>图中的 write，指的就是指把日志写入到 binlog 文件，但是并没有把数据持久化到磁盘，因为数据还缓存在文件系统的 page cache 里，write 的写入速度还是比较快的，因为不涉及磁盘 I/O。 图中的 fsync，才是将数据持久化到磁盘的操作，这里就会涉及磁盘 I/O，所以频繁的 fsync 会导致磁盘的 I/O 升高。</p><p>MySQL提供一个 sync_binlog 参数来控制数据库的 binlog 刷到磁盘上的频率：</p><ul><li>sync_binlog = 0 的时候，表示每次提交事务都只 write，不 fsync，后续交由操作系统决定何时将数据持久化到磁盘；</li><li>sync_binlog = 1 的时候，表示每次提交事务都会 write，然后马上执行 fsync；</li><li>sync_binlog =N(N&gt;1) 的时候，表示每次提交事务都 write，但累积 N 个事务后才 fsync。</li></ul><p>如果能容少量事务的 binlog 日志丢失的风险，为了提高写入的性能，一般会 sync_binlog 设置为 100~1000 中的某个数值。</p><h3 id="主从复制是怎么实现" tabindex="-1"><a class="header-anchor" href="#主从复制是怎么实现" aria-hidden="true">#</a> 主从复制是怎么实现？</h3><p>MySQL 的主从复制依赖于 binlog ，也就是记录 MySQL 上的所有变化并以二进制形式保存在磁盘上。复制的过程就是将 binlog 中的数据从主库传输到从库上。</p><p>这个过程一般是异步的，也就是主库上执行事务操作的线程不会等待复制 binlog 的线程同步完成。</p><p>MySQL 集群的主从复制过程梳理成 3 个阶段：</p><ul><li>写入 Binlog：主库写 binlog 日志，提交事务，并更新本地存储数据。</li><li>同步 Binlog：把 binlog 复制到所有从库上，每个从库把 binlog 写到暂存日志中。</li><li>回放 Binlog：回放 binlog，并更新存储引擎中的数据。</li></ul><blockquote><pre><code>MySQL 主从复制还有哪些模型？
</code></pre></blockquote><p>主要有三种：</p><ul><li>同步复制：MySQL 主库提交事务的线程要等待所有从库的复制成功响应，才返回客户端结果。这种方式在实际项目中，基本上没法用，原因有两个：一是性能很差，因为要复制到所有节点才返回响应；二是可用性也很差，主库和所有从库任何一个数据库出问题，都会影响业务。</li><li>异步复制（默认模型）：MySQL 主库提交事务的线程并不会等待 binlog 同步到各从库，就返回客户端结果。这种模式一旦主库宕机，数据就会发生丢失。</li><li>半同步复制：MySQL 5.7 版本之后增加的一种复制方式，介于两者之间，事务线程不用等待所有的从库复制成功响应，只要一部分复制成功响应回来就行，比如一主二从的集群，只要数据成功复制到任意一个从库上，主库的事务线程就可以返回给客户端。这种半同步复制的方式，兼顾了异步复制和同步复制的优点，即使出现主库宕机，至少还有一个从库有最新的数据，不存在数据丢失的风险。</li></ul><h3 id="为什么需要两阶段提交" tabindex="-1"><a class="header-anchor" href="#为什么需要两阶段提交" aria-hidden="true">#</a> 为什么需要两阶段提交？</h3><p>事务提交后，redo log 和 binlog 都要持久化到磁盘，但是这两个是独立的逻辑，可能出现半成功的状态，这样就造成两份日志之间的逻辑不一致。redo log 影响主库的数据，binlog 影响从库的数据，所以 redo log 和 binlog 必须保持一致才能保证主从数据一致。</p><p>MySQL 为了避免出现两份日志之间的逻辑不一致的问题，使用了「两阶段提交」来解决，两阶段提交其实是分布式事务一致性协议，它可以保证多个逻辑操作要不全部成功，要不全部失败，不会出现半成功的状态。</p><p>两阶段提交将单个事务提交拆分成了两个部分,分别是准备[Prepare]阶段和提交[Commit]阶段,每个阶段都由协调者和参与者组成,这里的提交(Commit)阶段和commit语句不同,commit 语句执行的时候，会包含提交（Commit）阶段。</p><blockquote><p>两阶段提交的过程是怎样的？</p></blockquote><p>在MySQL的InnoDB引擎下,开启binlog的情况下,MySQL内部会同时维护binlog日志与InnoDB的redo log,为了保证这两个日志的一致性,MySQL使用了内部XA事务,内部 XA 事务由 binlog 作为协调者，存储引擎是参与者。</p><p>当客户端执行 commit 语句或者在自动提交的情况下，MySQL 内部开启一个 XA 事务，分两阶段来完成 XA 事务的提交，如下图：</p><figure><img src="https://cdn.xiaolincoding.com/gh/xiaolincoder/mysql/how_update/两阶段提交.drawio.png?image_process=watermark,text_5YWs5LyX5Y-377ya5bCP5p6XY29kaW5n,type_ZnpsdHpoaw,x_10,y_10,g_se,size_20,color_0000CD,t_70,fill_0" alt="" tabindex="0" loading="lazy"><figcaption></figcaption></figure><p>从图中可以看出,提交过程有两个阶段,就是将redo log 的写入拆成了两个步骤：prepare 和 commit，中间再穿插写入binlog，具体如下：</p><ul><li>prepare 阶段：将 XID（内部 XA 事务的 ID） 写入到 redo log，同时将 redo log 对应的事务状态设置为 prepare，然后将 redo log 持久化到磁盘（innodb_flush_log_at_trx_commit = 1 的作用）</li><li>commit 阶段：把 XID 写入到 binlog，然后将 binlog 持久化到磁盘（sync_binlog = 1 的作用），接着调用引擎的提交事务接口，将 redo log 状态设置为 commit，此时该状态并不需要持久化到磁盘，只需要 write 到文件系统的 page cache 中就够了，因为只要 binlog 写磁盘成功，就算 redo log 的状态还是 prepare 也没有关系，一样会被认为事务已经执行成功</li></ul><blockquote><p>异常重启会出现什么现象？</p></blockquote><p>我们来看看在两阶段提交的不同时刻，MySQL 异常重启会出现什么现象？下图中有时刻 A 和时刻 B 都有可能发生崩溃：</p><figure><img src="https://cdn.xiaolincoding.com/gh/xiaolincoder/mysql/how_update/两阶段提交崩溃点.drawio.png?image_process=watermark,text_5YWs5LyX5Y-377ya5bCP5p6XY29kaW5n,type_ZnpsdHpoaw,x_10,y_10,g_se,size_20,color_0000CD,t_70,fill_0" alt="" tabindex="0" loading="lazy"><figcaption></figcaption></figure><p>不管是时刻 A（redo log 已经写入磁盘， binlog 还没写入磁盘），还是时刻 B （redo log 和 binlog 都已经写入磁盘，还没写入 commit 标识）崩溃，<strong>此时的 redo log 都处于 prepare 状态</strong>。</p><p>在 MySQL 重启后会按顺序扫描 redo log 文件，碰到处于 prepare 状态的 redo log，就拿着 redo log 中的 XID 去 binlog 查看是否存在此 XID：</p><ul><li>如果 binlog 中没有当前内部 XA 事务的 XID，说明 redolog 完成刷盘，但是 binlog 还没有刷盘，则回滚事务。对应时刻 A 崩溃恢复的情况。</li><li>如果 binlog 中有当前内部 XA 事务的 XID，说明 redolog 和 binlog 都已经完成了刷盘，则提交事务。对应时刻 B 崩溃恢复的情况。</li></ul><p>可以看到，对于处于 prepare 阶段的 redo log，即可以提交事务，也可以回滚事务，这取决于是否能在 binlog 中查找到与 redo log 相同的 XID，如果有就提交事务，如果没有就回滚事务。这样就可以保证 redo log 和 binlog 这两份日志的一致性了。</p><p>所以说，两阶段提交是以 binlog 写成功为事务提交成功的标识，因为 binlog 写成功了，就意味着能在 binlog 中查找到与 redo log 相同的 XID。</p><blockquote><p>处于 prepare 阶段的 redo log 加上完整 binlog，重启就提交事务，MySQL 为什么要这么设计?</p></blockquote><p>binlog 已经写入了，之后就会被从库（或者用这个 binlog 恢复出来的库）使用。</p><p>所以，在主库上也要提交这个事务。采用这个策略，主库和备库的数据就保证了一致性。</p><blockquote><p>事务没提交的时候，redo log 会被持久化到磁盘吗？</p></blockquote><p>会的。</p><p>事务执行中间过程的 redo log 也是直接写在 redo log buffer 中的，这些缓存在 redo log buffer 里的 redo log 也会被「后台线程」每隔一秒一起持久化到磁盘。</p><p>也就是说，事务没提交的时候，redo log 也是可能被持久化到磁盘的。</p><p>有的同学可能会问，如果 mysql 崩溃了，还没提交事务的 redo log 已经被持久化磁盘了，mysql 重启后，数据不就不一致了？</p><p>放心，这种情况 mysql 重启会进行回滚操作，因为事务没提交的时候，binlog 是还没持久化到磁盘的。</p><p>所以， redo log 可以在事务没提交之前持久化到磁盘，但是 binlog 必须在事务提交之后，才可以持久化到磁盘。</p><blockquote><p>两阶段提交有什么问题？</p></blockquote><p>两阶段提交虽然保证了两个日志文件的数据一致性，但是性能很差，主要有两个方面的影响：</p><ul><li>磁盘 I/O 次数高：对于“双1”配置，每个事务提交都会进行两次 fsync（刷盘），一次是 redo log 刷盘，另一次是 binlog 刷盘。</li><li>锁竞争激烈：两阶段提交虽然能够保证「单事务」两个日志的内容一致，但在「多事务」的情况下，却不能保证两者的提交顺序一致，因此，在两阶段提交的流程基础上，还需要加一个锁来保证提交的原子性，从而保证多事务的情况下，两个日志的提交顺序一致。</li></ul><blockquote><pre><code>为什么两阶段提交的磁盘 I/O 次数会很高？
</code></pre></blockquote><p>可以看到，如果 sync_binlog 和 当 innodb_flush_log_at_trx_commit 都设置为 1，那么在每个事务提交过程中， 都会至少调用 2 次刷盘操作，一次是 redo log 刷盘，一次是 binlog 落盘，所以这会成为性能瓶颈。</p><blockquote><pre><code>为什么锁竞争激烈？
</code></pre></blockquote><p>在早期的 MySQL 版本中，通过使用 prepare_commit_mutex 锁来保证事务提交的顺序，在一个事务获取到锁时才能进入 prepare 阶段，一直到 commit 阶段结束才能释放锁，下个事务才可以继续进行 prepare 操作。</p><p>通过加锁虽然完美地解决了顺序一致性的问题，但在并发量较大的时候，就会导致对锁的争用，性能不佳。</p><blockquote><p>组提交</p></blockquote><p>MySQL 引入了 binlog 组提交（group commit）机制，当有多个事务提交的时候，会将多个 binlog 刷盘操作合并成一个，从而减少磁盘 I/O 的次数，如果说 10 个事务依次排队刷盘的时间成本是 10，那么将这 10 个事务一次性一起刷盘的时间成本则近似于 1。</p><p>引入了组提交机制后，prepare 阶段不变，只针对 commit 阶段，将 commit 阶段拆分为三个过程：</p><ul><li>flush 阶段：多个事务按进入的顺序将 binlog 从 cache 写入文件（不刷盘）；</li><li>sync 阶段：对 binlog 文件做 fsync 操作（多个事务的 binlog 合并一次刷盘）；</li><li>commit 阶段：各个事务按顺序做 InnoDB commit 操作；</li></ul><blockquote><p>MySQL 磁盘 I/O 很高，有什么优化的方法？</p></blockquote><p>现在我们知道事务在提交的时候，需要将 binlog 和 redo log 持久化到磁盘，那么如果出现 MySQL 磁盘 I/O 很高的现象，我们可以通过控制以下参数，来 “延迟” binlog 和 redo log 刷盘的时机，从而降低磁盘 I/O 的频率：</p><ul><li>设置组提交的两个参数： binlog_group_commit_sync_delay 和 binlog_group_commit_sync_no_delay_count 参数，延迟 binlog 刷盘的时机，从而减少 binlog 的刷盘次数。这个方法是基于“额外的故意等待”来实现的，因此可能会增加语句的响应时间，但即使 MySQL 进程中途挂了，也没有丢失数据的风险，因为 binlog 早被写入到 page cache 了，只要系统没有宕机，缓存在 page cache 里的 binlog 就会被持久化到磁盘。</li><li>将 sync_binlog 设置为大于 1 的值（比较常见是 100~1000），表示每次提交事务都 write，但累积 N 个事务后才 fsync，相当于延迟了 binlog 刷盘的时机。但是这样做的风险是，主机掉电时会丢 N 个事务的 binlog 日志。</li><li>将 innodb_flush_log_at_trx_commit 设置为 2。表示每次事务提交时，都只是缓存在 redo log buffer 里的 redo log 写到 redo log 文件，注意写入到「 redo log 文件」并不意味着写入到了磁盘，因为操作系统的文件系统中有个 Page Cache，专门用来缓存文件数据的，所以写入「 redo log文件」意味着写入到了操作系统的文件缓存，然后交由操作系统控制持久化到磁盘的时机。但是这样做的风险是，主机掉电的时候会丢数据。</li></ul><p><span id="事务"></span></p><h2 id="事务" tabindex="-1"><a class="header-anchor" href="#事务" aria-hidden="true">#</a> 事务</h2><p>事务是由 MySQL 的引擎来实现的，我们常见的 InnoDB 引擎它是支持事务的。</p><p>事务必须遵循ACID</p><ol><li>原子性（Atomicity）：一个事务中的所有操作，要么全部完成，要么全部不完成，不会结束在中间某个环节，而且事务在执行过程中发生错误，会被回滚到事务开始前的状态，就像这个事务从来没有执行过一样.</li><li>一致性（Consistency）：是指事务操作前和操作后，数据满足完整性约束，数据库保持一致性状态。数据库在任何时候都保持一致性 —— 每一个的提交或回滚，包括事务进行的时候。如果多表更新相关数据，查询看到的结果要么全是旧值，要么全是新值，而不是旧值和新值的混合。</li><li>隔离性（Isolation）：数据库允许多个并发事务同时对其数据进行读写和修改的能力，隔离性可以防止多个事务并发执行时由于交叉执行而导致数据的不一致，因为多个事务同时使用相同的数据时，不会相互干扰，每个事务都有一个完整的数据空间，对其他并发事务是隔离的。</li><li>持久性（Durability）：事务处理结束后，对数据的修改就是永久的，即便系统故障也不会丢失。</li></ol><p>InnoDB 引擎通过什么技术来保证事务的这四个特性的呢？</p><ul><li>持久性是通过 redo log （重做日志）来保证的；</li><li>原子性是通过 undo log（回滚日志） 来保证的；</li><li>隔离性是通过 MVCC（多版本并发控制） 或锁机制来保证的；</li><li>一致性则是通过持久性+原子性+隔离性来保证；</li></ul><p>并行事务会引发什么问题？</p><p>MySQL 服务端是允许多个客户端连接的，这意味着 MySQL 会出现同时处理多个事务的情况。</p><p>那么在同时处理多个事务的时候，就可能出现脏读（dirty read）、不可重复读（non-repeatable read）、幻读（phantom read）的问题。</p><p><strong>脏读</strong>: 如果一个事务「读到」了另一个「未提交事务修改过的数据」，就意味着发生了「脏读」现象。 <strong>不可重复读</strong>: 在一个事务内多次读取同一个数据，如果出现前后两次读到的数据不一样的情况，就意味着发生了「不可重复读」现象。 <strong>幻读</strong>: 在一个事务内多次查询某个符合查询条件的「记录数量」，如果出现前后两次查询到的记录数量不一样的情况，就意味着发生了「幻读」现象。</p><p>事务的隔离级别有哪些?</p><p>SQL 标准提出了四种隔离级别来规避这些现象，隔离级别越高，性能效率就越低，这四个隔离级别如下：</p><ul><li>读未提交（read uncommitted），指一个事务还没提交时，它做的变更就能被其他事务看到；</li><li>读提交（read committed），指一个事务提交之后，它做的变更才能被其他事务看到；</li><li>可重复读（repeatable read），指一个事务执行过程中看到的数据，一直跟这个事务启动时看到的数据是一致的，MySQL InnoDB 引擎的默认隔离级别；</li><li>串行化（serializable ）；会对记录加上读写锁，在多个事务对这条记录进行读写操作时，如果发生了读写冲突的时候，后访问的事务必须等前一个事务执行完成，才能继续执行；</li></ul><figure><img src="https://img-blog.csdnimg.cn/img_convert/4e98ea2e60923b969790898565b4d643.png" alt="" tabindex="0" loading="lazy"><figcaption></figcaption></figure><p>针对不同的隔离级别，并发事务时可能发生的现象也会不同。</p><figure><img src="https://img-blog.csdnimg.cn/img_convert/4e98ea2e60923b969790898565b4d643.png" alt="" tabindex="0" loading="lazy"><figcaption></figcaption></figure><p>MySQL 在「可重复读」隔离级别下，可以很大程度上避免幻读现象的发生（注意是很大程度避免，并不是彻底避免），所以 MySQL 并不会使用「串行化」隔离级别来避免幻读现象的发生，因为使用「串行化」隔离级别会影响性能。</p><p>MySQL InnoDB 引擎的默认隔离级别虽然是「可重复读」，但是它很大程度上避免幻读现象，解决的方案有两种：</p><ul><li>针对快照读（普通 select 语句），是通过 MVCC 方式解决了幻读，因为可重复读隔离级别下，事务执行过程中看到的数据，一直跟这个事务启动时看到的数据是一致的，即使中途有其他事务插入了一条数据，是查询不出来这条数据的，所以就很好了避免幻读问题。</li><li>针对当前读（select ... for update 等语句），是通过 next-key lock（记录锁+间隙锁）方式解决了幻读，因为当执行 select ... for update 语句的时候，会加上 next-key lock，如果有其他事务在 next-key lock 锁范围内插入了一条记录，那么这个插入语句就会被阻塞，无法成功插入，所以就很好了避免幻读问题。</li></ul><h3 id="事务的隔离级别是怎么实现的" tabindex="-1"><a class="header-anchor" href="#事务的隔离级别是怎么实现的" aria-hidden="true">#</a> 事务的隔离级别是怎么实现的</h3><ul><li>对于「读未提交」隔离级别的事务来说，因为可以读到未提交事务修改的数据，所以直接读取最新的数据就好了；</li><li>对于「串行化」隔离级别的事务来说，通过加读写锁的方式来避免并行访问；</li><li>对于「读提交」和「可重复读」隔离级别的事务来说，它们是通过 Read View 来实现的，它们的区别在于创建 Read View 的时机不同，大家可以把 Read View 理解成一个数据快照，就像相机拍照那样，定格某一时刻的风景。「读提交」隔离级别是在「每个语句执行前」都会重新生成一个 Read View，而「可重复读」隔离级别是「启动事务时」生成一个 Read View，然后整个事务期间都在用这个 Read View。</li></ul><p>注意，执行「开始事务」命令，并不意味着启动了事务。在 MySQL 有两种开启事务的命令，分别是：</p><ul><li>第一种：begin/start transaction 命令；</li><li>第二种：start transaction with consistent snapshot 命令；</li></ul><p>这两种开启事务的命令，事务的启动时机是不同的：</p><ul><li>执行了 begin/start transaction 命令后，并不代表事务启动了。只有在执行这个命令后，执行了增删查改操作的 SQL 语句，才是事务真正启动的时机；</li><li>执行了 start transaction with consistent snapshot 命令，就会马上启动事务。</li></ul><p>接下来详细说下，Read View 在 MVCC 里如何工作的？</p><p>Read View 在 MVCC 里如何工作的？</p><p>我们需要了解两个知识：</p><ul><li>Read View 中四个字段作用；</li><li>聚簇索引记录中两个跟事务有关的隐藏列；</li></ul><p>那 Read View 到底是个什么东西？ <img src="https://cdn.xiaolincoding.com/gh/xiaolincoder/ImageHost4@main/mysql/事务隔离/readview结构.drawio.png" alt="" loading="lazy"></p><p>Read View里有四个重要字段:</p><ul><li>m_ids:指的是在创建Read View时,当前数据库中「<strong>活跃事务</strong>」的事务 id 列表，注意是一个列表，“活跃事务”指的就是，启动了但还没提交的事务。</li><li>min_trx_id ：指的是在创建 Read View 时，当前数据库中「<strong>活跃事务</strong>」中事务 id 最小的事务，也就是 m_ids 的最小值。</li><li>max_trx_id ：这个并不是 m_ids 的最大值，而是创建 Read View 时当前数据库中应该给下一个事务的 id 值，也就是全局事务中最大的事务 id 值 + 1；</li><li>creator_trx_id ：指的是创建该 Read View 的事务的事务 id。</li></ul><p>知道了 Read View 的字段，我们还需要了解聚簇索引记录中的两个隐藏列。</p><p>假设在账户余额表插入一条小林余额为 100 万的记录，然后我把这两个隐藏列也画出来，该记录的整个示意图如下：</p><figure><img src="https://img-blog.csdnimg.cn/img_convert/f595d13450878acd04affa82731f76c5.png" alt="" tabindex="0" loading="lazy"><figcaption></figcaption></figure><p>对于使用 InnoDB 存储引擎的数据库表，它的聚簇索引记录中都包含下面两个隐藏列：</p><ul><li>trx_id，当一个事务对某条聚簇索引记录进行改动时，就会把该事务的事务 id 记录在 trx_id 隐藏列里；</li><li>roll_pointer，每次对某条聚簇索引记录进行改动时，都会把旧版本的记录写入到 undo 日志中，然后这个隐藏列是个指针，指向每一个旧版本记录，于是就可以通过它找到修改前的记录。</li></ul><p>在创建 Read View 后，我们可以将记录中的 trx_id 划分这三种情况：</p><figure><img src="https://cdn.xiaolincoding.com/gh/xiaolincoder/ImageHost4@main/mysql/事务隔离/ReadView.drawio.png" alt="" tabindex="0" loading="lazy"><figcaption></figcaption></figure><ul><li>如果记录的 trx_id 值小于 Read View 中的 min_trx_id 值，表示这个版本的记录是在创建 Read View 前已经提交的事务生成的，所以该版本的记录对当前事务可见。</li><li>如果记录的 trx_id 值大于等于 Read View 中的 max_trx_id 值，表示这个版本的记录是在创建 Read View 后才启动的事务生成的，所以该版本的记录对当前事务不可见。</li><li>如果记录的 trx_id 值在 Read View 的 min_trx_id 和 max_trx_id 之间，需要判断 trx_id 是否在 m_ids 列表中： <ol><li>如果记录的 trx_id 在 m_ids 列表中，表示生成该版本记录的活跃事务依然活跃着（还没提交事务），所以该版本的记录对当前事务不可见。</li><li>如果记录的 trx_id 不在 m_ids列表中，表示生成该版本记录的活跃事务已经被提交，所以该版本的记录对当前事务可见。</li></ol></li></ul><p>这种通过「版本链」来控制并发事务访问同一个记录时的行为就叫 MVCC（多版本并发控制）。</p><blockquote><p>可重复读是如何工作的？</p></blockquote><p>可重复读隔离级别是启动事务时生成一个 Read View，然后整个事务期间都在用这个 Read View。</p><blockquote><p>读提交是如何工作的？</p></blockquote><p>读提交隔离级别是在每次读取数据时，都会生成一个新的 Read View。也意味着，事务期间的多次读取同一条数据，前后两次读的数据可能会出现不一致，因为可能这期间另外一个事务修改了该记录，并提交了事务。</p><blockquote><p>MySQL 可重复读隔离级别，完全解决幻读了吗？</p></blockquote><p>什么是幻读？当同一个查询在不同的时间产生不同的结果集时，事务中就会出现所谓的幻象问题。例如，如果 SELECT 执行了两次，但第二次返回了第一次没有返回的行，则该行是“幻像”行。</p><p>MySQL InnoDB 引擎的默认隔离级别虽然是「可重复读」，但是它很大程度上避免幻读现象（并不是完全解决了），解决的方案有两种：</p><ul><li>针对快照读（普通 select 语句），是通过 MVCC 方式解决了幻读，因为可重复读隔离级别下，事务执行过程中看到的数据，一直跟这个事务启动时看到的数据是一致的，即使中途有其他事务插入了一条数据，是查询不出来这条数据的，所以就很好了避免幻读问题。</li><li>针对当前读（select ... for update 等语句），是通过 next-key lock（记录锁+间隙锁）方式解决了幻读，因为当执行 select ... for update 语句的时候，会加上 next-key lock，如果有其他事务在 next-key lock 锁范围内插入了一条记录，那么这个插入语句就会被阻塞，无法成功插入，所以就很好了避免幻读问题。</li></ul><p>这两个解决方案是很大程度上解决了幻读现象，但是还是有个别的情况造成的幻读现象是无法解决的。</p><p>当前读是如何避免幻读的？</p><p>MySQL 里除了普通查询是快照读，其他都是当前读，比如 update、insert、delete，这些语句执行前都会查询最新版本的数据，然后再做进一步的操作。</p><p>这很好理解，假设你要 update 一个记录，另一个事务已经 delete 这条记录并且提交事务了，这样不是会产生冲突吗，所以 update 的时候肯定要知道最新的数据。</p><p>另外，select ... for update 这种查询语句是当前读，每次执行的时候都是读取最新的数据。</p><blockquote><p>幻读被完全解决了吗？</p></blockquote><p>可重复读隔离级别下虽然很大程度上避免了幻读，但是还是没有能完全解决幻读。</p><p>第一个发生幻读现象的场景</p><p>第一个例子：对于快照读， MVCC 并不能完全避免幻读现象。因为当事务 A 更新了一条事务 B 插入的记录，那么事务 A 前后两次查询的记录条目就不一样了，所以就发生幻读。</p><p>第二个发生幻读现象的场景</p><p>除了上面这一种场景会发生幻读现象之外，还有下面这个场景也会发生幻读现象。</p><ul><li>T1 时刻：事务 A 先执行「快照读语句」：select * from t_test where id &gt; 100 得到了 3 条记录。</li><li>T2 时刻：事务 B 往插入一个 id= 200 的记录并提交；</li><li>T3 时刻：事务 A 再执行「当前读语句」 select * from t_test where id &gt; 100 for update 就会得到 4 条记录，此时也发生了幻读现象。</li></ul><p>要避免这类特殊场景下发生幻读的现象的话，就是尽量在开启事务之后，马上执行 select ... for update 这类当前读的语句，因为它会对记录加 next-key lock，从而避免其他事务插入一条新记录。</p><p><span id="锁"></span></p><h2 id="锁" tabindex="-1"><a class="header-anchor" href="#锁" aria-hidden="true">#</a> 锁</h2><p>MySQL有哪些锁?</p><h3 id="全局锁" tabindex="-1"><a class="header-anchor" href="#全局锁" aria-hidden="true">#</a> 全局锁</h3><p>要使用全局锁<code>flush tables with read lock</code>,整个数据库就处于只读状态了,这时候其他线程执行对数据的增删改操作，比如 insert、delete、update等语句,对表结构的更改操作，比如 alter table、drop table 等语句,都会被阻塞。</p><p>如果要释放全局锁，则要执行这条命令：<code>unlock tables</code></p><p>当然，当会话断开了，全局锁会被自动释放。</p><p>全局锁主要应用于做全库逻辑备份，这样在备份数据库期间，不会因为数据或表结构的更新，而出现备份文件的数据与预期的不一样。</p><blockquote><p>既然备份数据库数据的时候，使用全局锁会影响业务，那有什么其他方式可以避免？ 如果数据库的引擎支持的事务支持可重复读的隔离级别，那么在备份数据库之前先开启事务，会先创建 Read View，然后整个事务执行期间都在用这个 Read View，而且由于 MVCC 的支持，备份期间业务依然可以对数据进行更新操作。因为在可重复读的隔离级别下，即使其他事务更新了表的数据，也不会影响备份数据库时的 Read View，这就是事务四大特性中的隔离性，这样备份期间备份的数据一直是在开启事务时的数据。</p></blockquote><h3 id="表级锁" tabindex="-1"><a class="header-anchor" href="#表级锁" aria-hidden="true">#</a> 表级锁</h3><p>MySQL 里面表级别的锁有这几种：</p><ul><li>表锁；</li><li>元数据锁（MDL）;</li><li>意向锁；</li><li>AUTO-INC 锁；</li></ul><h4 id="表锁" tabindex="-1"><a class="header-anchor" href="#表锁" aria-hidden="true">#</a> 表锁</h4><p>如果我们想对学生表（t_student）加表锁，可以使用下面的命令：</p><div class="language-text line-numbers-mode" data-ext="text"><pre class="language-text"><code>//表级别的共享锁，也就是读锁；
lock tables t_student read;

//表级别的独占锁，也就是写锁；
lock tables t_stuent write;
</code></pre><div class="line-numbers" aria-hidden="true"><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div></div></div><p>需要注意的是，表锁除了会限制别的线程的读写外，也会限制本线程接下来的读写操作。</p><p>也就是说如果本线程对学生表加了「共享表锁」，那么本线程接下来如果要对学生表执行写操作的语句，是会被阻塞的，当然其他线程对学生表进行写操作时也会被阻塞，直到锁被释放。</p><p>要释放表锁，可以使用下面这条命令，会释放当前会话的所有表锁：<code>unlock tables</code> 另外，当会话退出后，也会释放所有表锁。</p><p>不过尽量避免在使用 InnoDB 引擎的表使用表锁，因为表锁的颗粒度太大，会影响并发性能，InnoDB 牛逼的地方在于实现了颗粒度更细的行级锁。</p><h4 id="元数据锁" tabindex="-1"><a class="header-anchor" href="#元数据锁" aria-hidden="true">#</a> 元数据锁</h4><p>元数据锁(MDL)不需要我们显式去加,它在我们对数据表进行操作时会自动加,对一张表进行 CRUD 操作时，加的是 MDL 读锁,对一张表做结构变更操作的时候，加的是 MDL 写锁；MDL 是为了保证当用户对表执行 CRUD 操作时，防止其他线程对这个表结构做了变更。</p><p>当有线程在执行 select 语句（ 加 MDL 读锁）的期间，如果有其他线程要更改该表的结构（ 申请 MDL 写锁），那么将会被阻塞，直到执行完 select 语句（ 释放 MDL 读锁）。反之，当有线程对表结构进行变更（ 加 MDL 写锁）的期间，如果有其他线程执行了 CRUD 操作（ 申请 MDL 读锁），那么就会被阻塞，直到表结构变更完成（ 释放 MDL 写锁）。</p><blockquote><pre><code>MDL 不需要显示调用，那它是在什么时候释放的?
</code></pre></blockquote><p>MDL 是在事务提交后才会释放，这意味着事务执行期间，MDL 是一直持有的。</p><h4 id="意向锁" tabindex="-1"><a class="header-anchor" href="#意向锁" aria-hidden="true">#</a> 意向锁</h4><ul><li>在使用 InnoDB 引擎的表里对某些记录加上「共享锁」之前，需要先在表级别加上一个「意向共享锁」；</li><li>在使用 InnoDB 引擎的表里对某些纪录加上「独占锁」之前，需要先在表级别加上一个「意向独占锁」；</li></ul><p>也就是，当执行插入、更新、删除操作，需要先对表加上「意向独占锁」，然后对该记录加独占锁。</p><p>而普通的 select 是不会加行级锁的，普通的 select 语句是利用 MVCC 实现一致性读，是无锁的。</p><p>不过，select 也是可以对记录加共享锁和独占锁的，具体方式如下：<code>select ... lock in share mode;</code>, <code>select ... for update;</code></p><p>意向共享锁和意向独占锁是表级锁，不会和行级的共享锁和独占锁发生冲突，而且意向锁之间也不会发生冲突，只会和共享表锁（lock tables ... read）和独占表锁（lock tables ... write）发生冲突。</p><p>表锁和行锁是满足读读共享、读写互斥、写写互斥的。</p><p>如果没有「意向锁」，那么加「独占表锁」时，就需要遍历表里所有记录，查看是否有记录存在独占锁，这样效率会很慢。</p><p>那么有了「意向锁」，由于在对记录加独占锁前，先会加上表级别的意向独占锁，那么在加「独占表锁」时，直接查该表是否有意向独占锁，如果有就意味着表里已经有记录被加了独占锁，这样就不用去遍历表里的记录。</p><p>所以，意向锁的目的是为了快速判断表里是否有记录被加锁。</p><h4 id="auto-inc-锁" tabindex="-1"><a class="header-anchor" href="#auto-inc-锁" aria-hidden="true">#</a> AUTO-INC 锁</h4><p>表里的主键通常都会设置成自增的，这是通过对主键字段声明 <code>AUTO_INCREMENT</code> 属性实现的。</p><p>之后可以在插入数据时，可以不指定主键的值，数据库会自动给主键赋值递增的值，这主要是通过 AUTO-INC 锁实现的。AUTO-INC 锁是特殊的表锁机制，锁不是再一个事务提交后才释放，而是再执行完插入语句后就会立即释放。在插入数据时，会加一个表级别的 AUTO-INC 锁，然后为被 AUTO_INCREMENT 修饰的字段赋值递增的值，等插入语句执行完成后，才会把 AUTO-INC 锁释放掉。那么，一个事务在持有 AUTO-INC 锁的过程中，其他事务的如果要向该表插入语句都会被阻塞，从而保证插入数据时，被 AUTO_INCREMENT 修饰的字段的值是连续递增的。但是， AUTO-INC 锁再对大量数据进行插入的时候，会影响插入性能，因为另一个事务中的插入会被阻塞。</p><p>所以InnoDB 存储引擎提供了一种轻量级的锁来实现自增。一样也是在插入数据的时候，会为被 AUTO_INCREMENT 修饰的字段加上轻量级锁，然后给该字段赋值一个自增的值，就把这个轻量级锁释放了，而不需要等待整个插入语句执行完后才释放锁。</p><p>InnoDB 存储引擎提供了个 innodb_autoinc_lock_mode 的系统变量，是用来控制选择用 AUTO-INC 锁，还是轻量级的锁。</p><ul><li>当 innodb_autoinc_lock_mode = 0，就采用 AUTO-INC 锁，语句执行结束后才释放锁；</li><li>当 innodb_autoinc_lock_mode = 2，就采用轻量级锁，申请自增主键后就释放锁，并不需要等语句执行后才释放。</li><li>当 innodb_autoinc_lock_mode = 1： <ul><li>普通 insert 语句，自增锁在申请之后就马上释放；</li><li>类似 insert … select 这样的批量插入数据的语句，自增锁还是要等语句结束后才被释放；</li></ul></li></ul><h3 id="行级锁" tabindex="-1"><a class="header-anchor" href="#行级锁" aria-hidden="true">#</a> 行级锁</h3><p>InnoDB 引擎是支持行级锁的，而 MyISAM 引擎并不支持行级锁。</p><p>前面也提到，普通的 select 语句是不会对记录加锁的，因为它属于快照读。如果要在查询时对记录加行锁，可以使用下面这两个方式，这种查询会加锁的语句称为锁定读。</p><div class="language-text line-numbers-mode" data-ext="text"><pre class="language-text"><code>//对读取的记录加共享锁
select ... lock in share mode;

//对读取的记录加独占锁
select ... for update;
</code></pre><div class="line-numbers" aria-hidden="true"><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div></div></div><p>上面这两条语句必须在一个事务中，因为当事务提交了，锁就会被释放，所以在使用这两条语句的时候，要加上 begin、start transaction 或者 set autocommit = 0。</p><p>共享锁（S锁）满足读读共享，读写互斥。独占锁（X锁）满足写写互斥、读写互斥。</p><p>行级锁的类型主要有三类：</p><ul><li>Record Lock，记录锁，也就是仅仅把一条记录锁上；</li><li>Gap Lock，间隙锁，锁定一个范围，但是不包含记录本身；</li><li>Next-Key Lock：Record Lock + Gap Lock 的组合，锁定一个范围，并且锁定记录本身。</li></ul><h4 id="record-lock" tabindex="-1"><a class="header-anchor" href="#record-lock" aria-hidden="true">#</a> Record Lock</h4><p>Record Lock 称为记录锁，锁住的是一条记录。而且记录锁是有 S 锁和 X 锁之分的：</p><ul><li>当一个事务对一条记录加了 S 型记录锁后，其他事务也可以继续对该记录加 S 型记录锁（S 型与 S 锁兼容），但是不可以对该记录加 X 型记录锁（S 型与 X 锁不兼容）;</li><li>当一个事务对一条记录加了 X 型记录锁后，其他事务既不可以对该记录加 S 型记录锁（S 型与 X 锁不兼容），也不可以对该记录加 X 型记录锁（X 型与 X 锁不兼容）。</li></ul><p>举个例子，当一个事务执行了下面这条语句：</p><div class="language-text line-numbers-mode" data-ext="text"><pre class="language-text"><code>mysql &gt; begin;
mysql &gt; select * from t_test where id = 1 for update;
</code></pre><div class="line-numbers" aria-hidden="true"><div class="line-number"></div><div class="line-number"></div></div></div><p>就是对 t_test 表中主键 id 为 1 的这条记录加上 X 型的记录锁，这样其他事务就无法对这条记录进行修改了。当事务执行 commit 后，事务过程中生成的锁都会被释放。</p><h4 id="gap-lock" tabindex="-1"><a class="header-anchor" href="#gap-lock" aria-hidden="true">#</a> Gap Lock</h4><p>Gap Lock 称为间隙锁，只存在于可重复读隔离级别，目的是为了解决可重复读隔离级别下幻读的现象。</p><p>假设，表中有一个范围 id 为（3，5）间隙锁，那么其他事务就无法插入 id = 4 这条记录了，这样就有效的防止幻读现象的发生。间隙锁虽然存在 X 型间隙锁和 S 型间隙锁，但是并没有什么区别，间隙锁之间是兼容的，即两个事务可以同时持有包含共同间隙范围的间隙锁，并不存在互斥关系，因为间隙锁的目的是防止插入幻影记录而提出的。</p><h4 id="next-key-lock" tabindex="-1"><a class="header-anchor" href="#next-key-lock" aria-hidden="true">#</a> Next-Key Lock</h4><p>Next-Key Lock 称为临键锁，是 Record Lock + Gap Lock 的组合，锁定一个范围，并且锁定记录本身。</p><p>假设，表中有一个范围 id 为（3，5] 的 next-key lock，那么其他事务即不能插入 id = 4 记录，也不能修改 id = 5 这条记录。</p><p>所以，next-key lock 即能保护该记录，又能阻止其他事务将新纪录插入到被保护记录前面的间隙中。</p><p>next-key lock 是包含间隙锁+记录锁的，如果一个事务获取了 X 型的 next-key lock，那么另外一个事务在获取相同范围的 X 型的 next-key lock 时，是会被阻塞的。</p><p>比如，一个事务持有了范围为 (1, 10] 的 X 型的 next-key lock，那么另外一个事务在获取相同范围的 X 型的 next-key lock 时，就会被阻塞。</p><p>虽然相同范围的间隙锁是多个事务相互兼容的，但对于记录锁，我们是要考虑 X 型与 S 型关系，X 型的记录锁与 X 型的记录锁是冲突的。</p><h4 id="插入意向锁" tabindex="-1"><a class="header-anchor" href="#插入意向锁" aria-hidden="true">#</a> 插入意向锁</h4><p>一个事务在插入一条记录的时候，需要判断插入位置是否已被其他事务加了间隙锁（next-key lock 也包含间隙锁）。</p><p>如果有的话，插入操作就会发生阻塞，直到拥有间隙锁的那个事务提交为止（释放间隙锁的时刻），在此期间会生成一个插入意向锁，表明有事务想在某个区间插入新记录，但是现在处于等待状态。</p><p>举个例子，假设事务 A 已经对表加了一个范围 id 为（3，5）间隙锁。</p><p>当事务 A 还没提交的时候，事务 B 向该表插入一条 id = 4 的新记录，这时会判断到插入的位置已经被事务 A 加了间隙锁，于是事物 B 会生成一个插入意向锁，然后将锁的状态设置为等待状态（PS：MySQL 加锁时，是先生成锁结构，然后设置锁的状态，如果锁状态是等待状态，并不是意味着事务成功获取到了锁，只有当锁状态为正常状态时，才代表事务成功获取到了锁），此时事务 B 就会发生阻塞，直到事务 A 提交了事务。</p><p>插入意向锁名字虽然有意向锁，但是它并不是意向锁，它是一种特殊的间隙锁，属于行级别锁。</p><p>如果说间隙锁锁住的是一个区间，那么「插入意向锁」锁住的就是一个点。因而从这个角度来说，插入意向锁确实是一种特殊的间隙锁。</p><p>插入意向锁与间隙锁的另一个非常重要的差别是：尽管「插入意向锁」也属于间隙锁，但两个事务却不能在同一时间内，一个拥有间隙锁，另一个拥有该间隙区间内的插入意向锁（当然，插入意向锁如果不在间隙锁区间内则是可以的）。</p><p><span id="BufferPool"></span></p><h2 id="buffer-pool" tabindex="-1"><a class="header-anchor" href="#buffer-pool" aria-hidden="true">#</a> Buffer Pool</h2><h3 id="为什么要有buffer-pool" tabindex="-1"><a class="header-anchor" href="#为什么要有buffer-pool" aria-hidden="true">#</a> 为什么要有Buffer Pool?</h3><p>虽然说MySQL的数据是存储在磁盘里的,但是也不能每次都从磁盘里读取数据,这样做的性能是极差的.想要提升查询性能,加一个缓存就行了.所以当数据从磁盘中取出来后,缓存内存,下次查询同样的数据的时候,直接从内存中读取.为此,Innodb存储引擎设计了一个缓冲池(buffer pool),来提高数据库的读写性能.</p><p>所以MySQL在存储引擎层面有Buffer Pool可以提升性能.</p><p>有了缓冲池之后:</p><ul><li>当读取数据的时候,如果数据直接存在Buffer Pool,就直接去Buffer Pool读取</li><li>当修改数据时,首先修改Buffer Pool中所存在的页,然后将其设置为脏页,最后由后台线程将脏页写入到磁盘</li></ul><p>Buffer Pool默认128MB,也可以通过<code>innodb_buffer_pool_size</code>参数调整大小.</p><p>InnoDB会把存储中的数据划分为若干页,数据引擎读取是以页为单位进行的,一个页的大小是16KB,在MySQL启动时,InnoDB会向Buffer Pool申请一片连续的内存空间,然后按照默认的16KB的大小划分出一个个的页， Buffer Pool 中的页就叫做缓存页。此时这些缓存页都是空闲的(Free List),之后程序不断去运行,才会有磁盘中的页面加载入Buffer Pool中.</p><p>Buffer Pool中除了缓存索引页和数据页,还缓存undo页,插入缓存,自适应哈希索引、锁信息等等。</p><p>为了更好管理这些在Buffer Pool中的缓存页,InnoDB为每一个缓存页面都创建了控制块,控制块信息包括(缓存页的表空间,页号,缓存页地址,链表节点)等.控制块也是有占有空间的,它是放在Buffer Pool的最前面,接着才是缓存页.</p><figure><img src="`+t+`" alt="" tabindex="0" loading="lazy"><figcaption></figcaption></figure><p>中间的部分叫碎片空间,为什么会有碎片空间呢?因为可能控制块分配到最后剩下的最后一点点分配不到了,就产生碎片了.</p><p>我们查询一条记录的时候,InnoDB会把整个页的数据都加载到Buffer Pool中,因为索引只能定位到磁盘中的页,不能定位到页中的具体记录,将页加载到 Buffer Pool 后，再通过页里的页目录去定位到某条具体的记录。</p><h3 id="如何管理buffer-pool" tabindex="-1"><a class="header-anchor" href="#如何管理buffer-pool" aria-hidden="true">#</a> 如何管理Buffer Pool?</h3><p>如何管理空闲页?</p><p>Buffer Pool是一片连续的内存空间,还记得我们刚刚说过的MySQL刚开始运行时会加载一些空闲的缓冲页吗?当MySQL运行了一段时间后,这些页面中既有空闲的也还有被使用过的.当我们从磁盘中读取数据时,不会遍历这一片连续的内存空间去找到空闲的缓冲页面换掉,这样效率太低.</p><p>所以设计MySQL的人为了尽快找到空闲的缓冲页面,可以直接用链表结构将空闲的缓冲页作为链表节点,这个链表就叫Free 链表.</p><p>Free链表上也有控制块,还有一个头节点,该头节点包含链表的头节点地址，尾节点地址，以及当前链表中节点的数量等信息。</p><p>Free链表节点是一个个控制块,而每个控制块包含着对应的缓冲页地址,每当需要从磁盘缓存页面时,就从Free链表中取一个空闲的缓存页,并且把该缓存页对应的控制块的信息填上,然后把该缓存页对应的控制块从Free链表中移除.</p><p>如何管理脏页?</p><p>设计Buffer pool除了能提高读性能,还能提高写性能,当我们需要修改数据时,并不是每次都直接改磁盘数据,而是将Buffer Pool中将该被修改的页面标记为脏页,然后后台线程将脏页写入到磁盘中.那为了能够快速知道哪些缓存页是脏的,于是设计出了Flush链表,它跟Free链表类似,链表的节点也是控制块,区别在于Flush链表都是脏页.</p><p>有了 Flush 链表后，后台线程就可以遍历 Flush 链表，将脏页写入到磁盘。</p><p>如何提高缓冲命中率?</p><p>Buffer Pool大小是有限的,对于频繁访问的数据,我们希望它一直留在Buffer Pool中,而另一些访问很少的数据,也希望淘汰掉,所以LRU算法是最合适的.思路是链表头节点是最近使用的,而链表尾节点是最久最没被使用的.那么当空间不够了就淘汰最久最没被使用的.</p><p>简单的 LRU 算法的实现思路是这样的：</p><p>当访问的页面不再Buffer Pool中时,就直接把该页面对应的LRU链表节点移动到链表头部,当访问页不在Buffer Pool中时,除了除了要把页放入到 LRU 链表的头部，还要淘汰 LRU 链表末尾的节点。</p><p>到这里我们可以知道，Buffer Pool 里有三种页和链表来管理数据。</p><ul><li>Free Page（空闲页），表示此页未被使用，位于 Free 链表；</li><li>Clean Page（干净页），表示此页已被使用，但是页面未发生修改，位于LRU 链表。</li><li>Dirty Page（脏页），表示此页「已被使用」且「已经被修改」，其数据和磁盘上的数据已经不一致。当脏页上的数据写入磁盘后，内存数据和磁盘数据一致，那么该页就变成了干净页。脏页同时存在于 LRU 链表和 Flush 链表。</li></ul><p>简单的 LRU 算法并没有被 MySQL 使用，因为简单的 LRU 算法无法避免下面这两个问题：</p><ol><li>预读失效；</li><li>Buffer Pool 污染；</li></ol><p>什么是预读失败?</p><p>MySQL 在加载数据页时，会提前把它相邻的数据页一并加载进来，目的是为了减少磁盘 IO。但是可能这些被提前加载进来的数据页，并没有被访问，相当于这个预读是白做了，这个就是<strong>预读失效</strong>。如果使用简单的 LRU 算法，就会把预读页放到 LRU 链表头部，而当 Buffer Pool空间不够的时候，还需要把末尾的页淘汰掉。</p><p>如果这些预读页如果一直不会被访问到，就会出现一个很奇怪的问题，不会被访问的预读页却占用了 LRU 链表前排的位置，而末尾淘汰的页，可能是频繁访问的页，这样就大大降低了缓存命中率。</p><p>那怎么解决预读失败而导致的缓存命中率较低的问题呢?</p><p>我们不能因为害怕预读失效，而将预读机制去掉，大部分情况下，局部性原理还是成立的。</p><p>要避免预读失效带来影响，最好就是让预读的页停留在 Buffer Pool 里的时间要尽可能的短，让真正被访问的页才移动到 LRU 链表的头部，从而保证真正被读取的热数据留在 Buffer Pool 里的时间尽可能长。</p><p>那到底怎么才能避免呢？</p><p>MySQL 是这样做的，它改进了 LRU 算法，将 LRU 划分了 2 个区域：old 区域 和 young 区域。</p><p>young 区域在 LRU 链表的前半部分，old 区域则是在后半部分，如下图：</p><figure><img src="https://cdn.xiaolincoding.com/gh/xiaolincoder/ImageHost4@main/mysql/innodb/young%2Bold.png" alt="" tabindex="0" loading="lazy"><figcaption></figcaption></figure><p>old 区域占整个 LRU 链表长度的比例可以通过 innodb_old_blocks_pc 参数来设置，默认是 37，代表整个 LRU 链表中 young 区域与 old 区域比例是 63:37。</p><p>划分这两个区域后，预读的页就只需要加入到 old 区域的头部，当页被真正访问的时候，才将页插入 young 区域的头部。如果预读的页一直没有被访问，就会从 old 区域移除，这样就不会影响 young 区域中的热点数据。</p><p>什么是 Buffer Pool 污染？</p><p>当某一个 SQL 语句扫描了大量的数据时，在 Buffer Pool 空间比较有限的情况下，可能会将 Buffer Pool 里的所有页都替换出去，导致大量热数据被淘汰了，等这些热数据又被再次访问的时候，由于缓存未命中，就会产生大量的磁盘 IO，MySQL 性能就会急剧下降，这个过程被称为 Buffer Pool 污染。</p><p>注意， Buffer Pool 污染并不只是查询语句查询出了大量的数据才出现的问题，即使查询出来的结果集很小，也会造成 Buffer Pool 污染。</p><p>怎么解决出现 Buffer Pool 污染而导致缓存命中率下降的问题？</p><p>像前面这种全表扫描的查询，很多缓冲页其实只会被访问一次，但是它却只因为被访问了一次而进入到 young 区域，从而导致热点数据被替换了。</p><p>LRU 链表中 young 区域就是热点数据，只要我们提高进入到 young 区域的门槛，就能有效地保证 young 区域里的热点数据不会被替换掉。</p><p>MySQL 是这样做的，进入到 young 区域条件增加了一个停留在 old 区域的时间判断。</p><p>具体是这样做的，在对某个处在 old 区域的缓存页进行第一次访问时，就在它对应的控制块中记录下来这个访问时间：</p><pre><code>如果后续的访问时间与第一次访问的时间在某个时间间隔内，那么该缓存页就不会被从 old 区域移动到 young 区域的头部；
如果后续的访问时间与第一次访问的时间不在某个时间间隔内，那么该缓存页移动到 young 区域的头部；
</code></pre><p>这个间隔时间是由 innodb_old_blocks_time 控制的，默认是 1000 ms。</p><p>也就说，只有同时满足「被访问」与「在 old 区域停留时间超过 1 秒」两个条件，才会被插入到 young 区域头部，这样就解决了 Buffer Pool 污染的问题 。</p><p>另外，MySQL 针对 young 区域其实做了一个优化，为了防止 young 区域节点频繁移动到头部。young 区域前面 1/4 被访问不会移动到链表头部，只有后面的 3/4被访问了才会。</p><p>脏页什么时候会被刷入磁盘?</p><p>引入了 Buffer Pool 后，当修改数据时，首先是修改 Buffer Pool 中数据所在的页，然后将其页设置为脏页，但是磁盘中还是原数据。</p><p>因此，脏页需要被刷入磁盘，保证缓存和磁盘数据一致，但是若每次修改数据都刷入磁盘，则性能会很差，因此一般都会在一定时机进行批量刷盘。</p><p>可能大家担心，如果在脏页还没有来得及刷入到磁盘时，MySQL 宕机了，不就丢失数据了吗？</p><p>这个不用担心，InnoDB 的更新操作采用的是 Write Ahead Log 策略，即先写日志，再写入磁盘，通过 redo log 日志让 MySQL 拥有了崩溃恢复能力。</p><p>下面几种情况会触发脏页的刷新：</p><ul><li>当 redo log 日志满了的情况下，会主动触发脏页刷新到磁盘；</li><li>Buffer Pool 空间不足时，需要将一部分数据页淘汰掉，如果淘汰的是脏页，需要先将脏页同步到磁盘；</li><li>MySQL 认为空闲时，后台线程回定期将适量的脏页刷入到磁盘；</li><li>MySQL 正常关闭之前，会把所有的脏页刷入到磁盘；</li></ul><p>在我们开启了慢 SQL 监控后，如果你发现**「偶尔」会出现一些用时稍长的 SQL**，这可能是因为脏页在刷新到磁盘时可能会给数据库带来性能开销，导致数据库操作抖动。</p><p>如果间断出现这种现象，就需要调大 Buffer Pool 空间或 redo log 日志的大小。</p>`,320);function k(M,B){const l=p("ExternalLinkIcon");return a(),d("div",null,[g,e("p",null,[e("a",u,[o("执行图"),i(l)])]),s,e("p",null,[e("a",h,[o("innodb-是如何存储数据的"),i(l)])]),e("p",null,[e("a",b,[o("为什么 MySQL 采用 B+ 树作为索引？"),i(l)])]),e("p",null,[e("a",f,[o("索引失效有哪些？"),i(l)])]),y,_,e("p",null,[e("a",m,[o("Buffer Pool"),i(l)])]),L,e("p",null,[e("a",x,[o("日志"),i(l)])]),S])}const q=r(c,[["render",k],["__file","mysql-notes.html.vue"]]);export{q as default};
